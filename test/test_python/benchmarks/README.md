# DoubleML.jl vs Python Benchmark

This directory contains a performance benchmark comparing DoubleML.jl (Julia) against the Python DoubleML package on identical data.

## Configuration

- **Data:** 500,000 observations with 1,000 covariates
- **Treatment Effect:** 0.5
- **Cross-fitting Folds:** 5
- **Julia Learner:** EvoTreeRegressor (100 trees, max_depth=6)
- **Python Learner:** XGBRegressor (100 trees, max_depth=6)

## Files

| File | Purpose |
|------|---------|
| `benchmark_julia.jl` | Julia benchmark script using BenchmarkTools |
| `benchmark_python.py` | Python benchmark script using time.perf_counter() |
| `generate_report.jl` | Generates `benchmarks.md` report from results |
| `run_benchmark.sh` | Master script that runs all benchmarks in sequence |
| `benchmark_data.csv` | Shared dataset (generated by Julia script) |
| `benchmark_results_julia.json` | Julia timing and coefficient results |
| `benchmark_results_python.json` | Python timing and coefficient results |
| `benchmarks.md` | Human-readable benchmark report |

## Usage

### Quick Start

Run the complete benchmark with one command:

```bash
./run_benchmark.sh
```

This will:
1. Generate 500K x 1000 data in Julia
2. Save to CSV for Python
3. Run Julia benchmark with BenchmarkTools (5 samples)
4. Run Python benchmark (5 repetitions)
5. Generate comparison report

### Individual Steps

If you prefer to run steps individually:

**Step 1: Julia benchmark (generates data + benchmarks)**
```bash
cd ../../..
julia --project=. test/test_python/benchmarks/benchmark_julia.jl
```

**Step 2: Python benchmark**
```bash
cd test/test_python/benchmarks
python benchmark_python.py
```

**Step 3: Generate report**
```bash
cd ../../..
julia --project=. test/test_python/benchmarks/generate_report.jl
```

### View Results

After running, view the report:

```bash
cat test/test_python/benchmarks/benchmarks.md
```

## Expected Runtime

With 500K observations and 1,000 dimensions:
- **Data generation:** ~10-20 seconds
- **Julia benchmark:** ~5-15 minutes (depending on hardware)
- **Python benchmark:** ~5-15 minutes (depending on hardware)
- **Total time:** ~10-30 minutes

## Requirements

### Julia
- `DoubleML.jl` (dev version from repo)
- `EvoTrees.jl`
- `BenchmarkTools.jl`
- `CSV.jl`, `DataFrames.jl`, `JSON3.jl`

### Python
- `doubleml>=0.11.2`
- `xgboost`
- `pandas`
- `numpy`

## Output Format

The generated `benchmarks.md` includes:
- Test configuration
- Learner specifications
- Timing comparison (median, mean, std)
- Speedup factor
- Coefficient estimates comparison
- Accuracy assessment

## Notes

- Both implementations use the same `partialling_out` score function
- Julia uses `BenchmarkTools.jl` for accurate timing with setup/evaluation isolation
- Python uses `time.perf_counter()` with fresh model instances each repetition
- The shared CSV ensures both implementations see identical data
- Results may vary based on hardware, Julia/Python versions, and package versions
